{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainable AI for Medical Diagnostics: Reference Implementation\n",
    "\n",
    "This notebook contains the architectural definitions and functional logic for the models described in the paper **\"Sustainable AI for Medical Diagnostics: A Multi-Objective Comparative Study\"**.\n",
    "\n",
    "**Note:** This is a **Minimal Working Example (MWE)** configured for code verification. It uses \"toy\" hyperparameters (e.g., 1 epoch, reduced simulation timesteps) to demonstrate pipeline functionality without requiring High Performance Computing (HPC) resources. It is **not** intended to reproduce the high-accuracy results (77.3%) or full emissions data presented in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import pennylane as qml\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "import numpy as np\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading (MedMNIST v2)\n",
    "Loads the OrganAMNIST dataset (Abdominal CT, 11 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'organamnist'\n",
    "info = INFO[DATASET]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load minimal subsets for the Demo\n",
    "train_ds = DataClass(split='train', transform=transform, download=True)\n",
    "test_ds = DataClass(split='test', transform=transform, download=True)\n",
    "\n",
    "# Small batch size for demo purposes\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "n_classes = len(info['label'])\n",
    "print(f\"Loaded {DATASET} with {n_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "This section defines the three core architectures compared in the study:\n",
    "1. **Classical CNN:** The production baseline.\n",
    "2. **Neuromorphic SNN:** The energy-efficient proposal.\n",
    "3. **Hybrid Quantum-Classical:** The experimental comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2A. Classical CNN ---\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(32 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- 2B. Neuromorphic SNN ---\n",
    "class SimpleSNN(nn.Module):\n",
    "    def __init__(self, n_out, beta=0.9):\n",
    "        super().__init__()\n",
    "        # No pooling to preserve spatial spikes, matching paper param count approx.\n",
    "        self.conv = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(16 * 28 * 28, n_out)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        spk2_rec = []\n",
    "\n",
    "        # Time-loop (SNN dynamics)\n",
    "        for step in range(x_seq.size(0)):\n",
    "            x = self.conv(x_seq[step])\n",
    "            spk1, mem1 = self.lif1(x, mem1)\n",
    "            x = self.flatten(spk1)\n",
    "            spk2, mem2 = self.lif2(self.fc(x), mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "        \n",
    "        return torch.stack(spk2_rec, dim=0)\n",
    "\n",
    "# --- 2C. Quantum Hybrid Head ---\n",
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def vqc_circuit(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(), nn.MaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 7 * 7, n_qubits) # Reduce to qubit count\n",
    "        )\n",
    "        self.weight_shapes = {\"weights\": (2, n_qubits)}\n",
    "        self.qlayer = qml.qnn.TorchLayer(vqc_circuit, self.weight_shapes)\n",
    "        self.fc = nn.Linear(n_qubits, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.qlayer(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Future Directions: Spectral & Compressed Sensing\n",
    "These modules implement the Wavelet (DWT), DCT, and Compressive Sensing blocks discussed in the \"Future Directions\" section of the paper. They are provided here for reproducibility of the extension experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "from scipy.fftpack import dct\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def dwt_block(x, wavelet='db2', level=1):\n",
    "    # Applies Discrete Wavelet Transform\n",
    "    # x: [B,1,H,W]\n",
    "    B = x.shape[0]\n",
    "    out = []\n",
    "    for i in range(B):\n",
    "        arr = x[i,0].cpu().numpy()\n",
    "        coeffs2 = pywt.wavedec2(arr, wavelet, level=level)\n",
    "        cA = coeffs2[0]\n",
    "        feat = torch.tensor(cA).float().unsqueeze(0)\n",
    "        out.append(feat)\n",
    "    return torch.stack(out, dim=0)\n",
    "\n",
    "def dct_block(x, keep=0.5):\n",
    "    # Applies Discrete Cosine Transform with masking\n",
    "    B,_,H,W = x.shape\n",
    "    out = np.zeros((B,1,H,W), dtype=np.float32)\n",
    "    for i in range(B):\n",
    "        arr = x[i,0].cpu().numpy()\n",
    "        dct2 = dct(dct(arr.T, type=2, norm='ortho').T, type=2, norm='ortho')\n",
    "        kH, kW = max(1,int(H*keep)), max(1,int(W*keep))\n",
    "        mask = np.zeros_like(dct2); mask[:kH,:kW]=1\n",
    "        out[i,0] = dct2 * mask\n",
    "    return torch.tensor(out)\n",
    "\n",
    "print(\"Experimental preprocessing blocks loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Energy Tracking & Demo Execution\n",
    "This section runs a single-epoch training loop to demonstrate the pipeline is functional. \n",
    "\n",
    "**Parameters used for Demo:**\n",
    "* `Epochs`: 1 (Paper used 50+)\n",
    "* `SNN Timesteps`: 10 (Paper used 20)\n",
    "* `Batch Size`: 16 (Paper used 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyContext:\n",
    "    def __init__(self, project_name):\n",
    "        self.tracker = EmissionsTracker(project_name=project_name, measure_power_secs=1, output_dir='.')\n",
    "    def __enter__(self):\n",
    "        self.tracker.start()\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.tracker.stop()\n",
    "\n",
    "def train_demo(model, loader, model_type='cnn', epochs=1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"--- Starting {model_type.upper()} Demo Run ---\")\n",
    "    \n",
    "    with EnergyContext(project_name=f\"demo_{model_type}\"):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_idx, (data, target) in enumerate(loader):\n",
    "                data, target = data.to(device), target.to(device).squeeze().long()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if model_type == 'snn':\n",
    "                    # Rate encoding for SNN\n",
    "                    spk_data = spikegen.rate(data, num_steps=10) # Reduced steps for demo\n",
    "                    spk_out = model(spk_data)\n",
    "                    # Sum spikes over time for classification\n",
    "                    output = spk_out.sum(dim=0)\n",
    "                else:\n",
    "                    output = model(data)\n",
    "                \n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Epoch {epoch+1} [{batch_idx}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
    "                    break # Stop early for demo\n",
    "\n",
    "# --- Execute Demos ---\n",
    "# 1. CNN Demo\n",
    "cnn = SmallCNN(n_classes=n_classes)\n",
    "train_demo(cnn, train_loader, model_type='cnn')\n",
    "\n",
    "# 2. SNN Demo\n",
    "snn_model = SimpleSNN(n_out=n_classes)\n",
    "train_demo(snn_model, train_loader, model_type='snn')\n",
    "\n",
    "print(\"\\nDemo execution complete. Energy logs saved to ./emissions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}